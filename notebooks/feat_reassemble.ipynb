{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dcc84d-2af4-4738-88a6-b07cfc619805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "SIZE = 216\n",
    "\n",
    "def fspecial_gauss(size: int, sigma: int) -> np.ndarray:\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "\n",
    "        Parameters:\n",
    "         size (int): size of square guassian kernel\n",
    "         sigma (float): diameter of the kernel\n",
    "    \n",
    "        Returns:\n",
    "         g (np.ndarray): gaussian kernel from [0, 1]\n",
    "    \"\"\"\n",
    "    x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "    return g\n",
    "\n",
    "\n",
    "def load_mosaic_feats(out_folder: str, depth) -> np.ndarray:\n",
    "    _SIZE = 216\n",
    "    \"\"\"\n",
    "    Loads the .npy subtile files in an output folder and mosaics the overlapping predictions\n",
    "    to return a single .npy file of tree cover for the 6x6 km tile\n",
    "    Additionally, applies post-processing threshold rules and implements no-data flag of 255\n",
    "    \n",
    "        Parameters:\n",
    "         out_folder (os.Path): location of the prediction .npy files \n",
    "    \n",
    "        Returns:\n",
    "         predictions (np.ndarray): 6 x 6 km tree cover data as a uint8 from 0-100 w/ 255 no-data flag\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the tiling schema\n",
    "    x_tiles = [x for x in os.listdir(out_folder) if x.isnumeric()]\n",
    "    x_tiles = [int(x) for x in x_tiles]\n",
    "    max_x = np.max(x_tiles) + _SIZE\n",
    "    for x_tile in x_tiles:\n",
    "        y_tiles = [y[:-4] for y in os.listdir(out_folder + str(x_tile) + \"/\") if '.DS' not in y]\n",
    "        y_tiles = [int(x.split(\"_\")[0]) for x in y_tiles]\n",
    "        #y_tiles = [int(y[:-4]) for y in os.listdir(out_folder + str(x_tile) + \"/\") if '.DS' not in y]\n",
    "        if len(y_tiles) > 0:\n",
    "            max_y = np.max(y_tiles) + _SIZE\n",
    "        \n",
    "    def _load_partial(out_folder, start, end, x_tiles = x_tiles, y_tiles = y_tiles, feats = False):\n",
    "        # For predictions, load up a (SIZE, SIZE) array, * by 100\n",
    "        # For features, load up a (SIZE, SIZE, 64) array and iterate thru 8 at a time\n",
    "        #     due to memory constraints.\n",
    "        # \n",
    "        MULT = 100\n",
    "\n",
    "        n = end - start\n",
    "        # 3 x 65 x 618 x 618,\n",
    "            # 65, 618, 618 array\n",
    "            # for i in range(0, 65, 8):\n",
    "                # 25 x (8, 200, 200)\n",
    "        predictions = np.full((n, max_x, max_y, len(x_tiles) * len(y_tiles)),\n",
    "                              np.nan, dtype = np.float32)\n",
    "        mults = np.full((1, max_x, max_y, len(x_tiles) * len(y_tiles)),\n",
    "                        0, dtype = np.float32)\n",
    "        i = 0\n",
    "        for x_tile in x_tiles:\n",
    "            y_tiles = [y[:-4] for y in os.listdir(out_folder + str(x_tile) + \"/\") if '.DS' not in y]\n",
    "            y_tiles = [int(x.split(\"_\")[0]) for x in y_tiles]\n",
    "            for y_tile in y_tiles:\n",
    "                output_file = out_folder + str(x_tile) + \"/\" + str(y_tile) + \"_middle.npy\"\n",
    "                if os.path.exists(output_file):\n",
    "                    prediction = np.load(output_file)\n",
    "                    if prediction.shape[0] == 216:\n",
    "                        GAUSS = 46\n",
    "                    elif prediction.shape[0] > 200:\n",
    "                        GUASS = 44\n",
    "                    if prediction.shape[0] == 190:\n",
    "                        GAUSS = 36\n",
    "                    if prediction.shape[0] <= 150:\n",
    "                        GAUSS = 30\n",
    "                    if prediction.shape[0] <= 100:\n",
    "                        GAUSS = 20\n",
    "                    else:\n",
    "                        GAUSS = 36\n",
    "                    GAUSS = 36\n",
    "                    #GAUSS = 46 if prediction.shape[0] > 150 else 5\n",
    "                    if n > 1 or feats:\n",
    "                        prediction = prediction[..., start:end]\n",
    "                    else:\n",
    "                        prediction[prediction < 255] = prediction[prediction < 255] * MULT\n",
    "                    if (np.sum(prediction) < _SIZE*_SIZE*255) or depth > 1:\n",
    "                        prediction = (prediction).T.astype(np.float32)\n",
    "                        predictions[:, x_tile: x_tile+_SIZE, y_tile:y_tile + _SIZE, i] = prediction\n",
    "                        fspecial_i = fspecial_gauss(_SIZE, GAUSS)\n",
    "                        if depth == 1:\n",
    "                            fspecial_i[prediction > 100] = 0.\n",
    "                        mults[:, x_tile: x_tile+_SIZE, y_tile:y_tile + _SIZE, i] = fspecial_i # or 44\n",
    "\n",
    "                    i += 1\n",
    "            \n",
    "        if depth > 1:\n",
    "            predictions = predictions.astype(np.float32)\n",
    "            mults = mults / np.sum(mults, axis = -1)[..., np.newaxis]\n",
    "            predictions = np.nansum(predictions * mults, axis = -1)\n",
    "            predictions = np.int16(predictions)\n",
    "            return predictions\n",
    "        else:\n",
    "            mults = mults.squeeze()\n",
    "            predictions = predictions.astype(np.float32)\n",
    "            predictions = predictions.squeeze()\n",
    "\n",
    "            ratios = np.zeros((predictions.shape[-1]), dtype = np.float32)\n",
    "            mults[np.isnan(predictions)] = 0.\n",
    "            try:\n",
    "                for i in range(predictions.shape[-1]):\n",
    "                    ratios[i] = calc_overlap(i, predictions)\n",
    "                multipliers = np.median(ratios) / ratios\n",
    "                multipliers[multipliers > 1.5] = 1.5\n",
    "                for i in range(predictions.shape[-1]):\n",
    "                    mults[..., i] *= multipliers[i]\n",
    "            except:\n",
    "                print(\"Skipping the weighted average due to cloud cover\")\n",
    "\n",
    "            predictions[predictions > 100] = np.nan\n",
    "            \n",
    "            mults = mults / np.sum(mults, axis = -1)[..., np.newaxis]\n",
    "\n",
    "            out = np.copy(predictions)\n",
    "            out = np.sum(np.isnan(out), axis = (2))\n",
    "            n_preds = predictions.shape[-1]\n",
    "            predictions = np.nansum(predictions * mults, axis = -1)\n",
    "            predictions[out == n_preds] = np.nan\n",
    "            predictions[np.isnan(predictions)] = 255.\n",
    "            predictions = predictions.astype(np.uint8)\n",
    "\n",
    "            predictions[predictions <= .15*MULT] = 0.        \n",
    "            predictions[predictions > 100] = 255.\n",
    "        \n",
    "            return predictions\n",
    "    \n",
    "    output = np.full((depth, max_x, max_y),\n",
    "                              0., dtype = np.int16)\n",
    "    if depth == 1:\n",
    "        output = _load_partial(out_folder, 1, 2)\n",
    "    else:\n",
    "        iters = np.arange(0, depth, 8)\n",
    "        for i in iters:\n",
    "            incrementer = np.min((depth - i, 8))\n",
    "            print(f\"The incrementer is {incrementer} for iter {i}\")\n",
    "            preds = _load_partial(out_folder, i, i + incrementer, feats = True)\n",
    "            output[i:i + incrementer] = preds\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84f9d5-9f07-4f1b-9f42-6d77870af8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = load_mosaic_feats('../project-monitoring/tiles/1656/1130/feats/', depth = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f33ab-b38a-4407-a2cd-e9436a8b3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle = np.load(\"../src/middle.npy\")\n",
    "#middle = np.moveaxis(middle, 1, 2)\n",
    "\n",
    "sns.heatmap(middle[0].T)\n",
    "print(middle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1747d2-5c45-419b-a4b7-c8552ac672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "left = '../project-monitoring/tiles/1656/1130/raw/feats/1656X1130Y_feats.hkl'\n",
    "left = hkl.load(left)\n",
    "left = np.moveaxis(left, 1, 2)\n",
    "sns.heatmap(left[0])\n",
    "print(left.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040386ea-56ec-4f2b-a499-2ec5c0ce3cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "right =  '../project-monitoring/tiles/1657/1130/raw/feats/1657X1130Y_feats.hkl'\n",
    "right = hkl.load(right)\n",
    "right = np.moveaxis(right, 1, 2)\n",
    "\n",
    "sns.heatmap(right[0])\n",
    "print(right.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938b755-682f-467f-9ad6-b31f751a160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_s = (lr.shape[2] - middle.shape[2]) // 2\n",
    "print(left_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150093b7-e53d-48a6-8e9c-41a5846591d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = np.concatenate([left, right], axis = 2)\n",
    "plt.figure(figsize =(20, 5))\n",
    "#lr[1, :, left_s:left_s + 618] = lr[1, :, left_s:left_s + 618] + middle[1].T\n",
    "#lr[1, :, left_s:left_s + 618] = lr[1, :, left_s:left_s + 618] / 2\n",
    "#lr[1] /= 2\n",
    "sns.heatmap(lr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa521c7-b0c2-4181-ae1e-16b68a4f52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(middle[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e8f9c-68f0-4b60-a34b-328737ef7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "def update_ard_tiles(x, y, m, base_path = '../project-monitoring/tiles/'):\n",
    "    # Step 1 check if ARD exists\n",
    "    # Step 2 update the \n",
    "\n",
    "    left_local_path = f'{base_path}{str(x)}/{str(y)}/raw/feats/{str(x)}X{str(y)}Y_feats.hkl'\n",
    "    right_local_path = f'{base_path}{str(int(x) + 1)}/{str(y)}/raw/feats/{str(int(x) + 1)}X{str(y)}Y_feats.hkl'\n",
    "    print(\"Loading original features\")\n",
    "    print(left_local_path)\n",
    "    print(right_local_path)\n",
    "    \n",
    "    l = hkl.load(left_local_path) / 32768\n",
    "    r = hkl.load(right_local_path) / 32768\n",
    "    m = m / 32768\n",
    "    l = np.moveaxis(l, 1, 2)\n",
    "    r = np.moveaxis(r, 1, 2)\n",
    "    m = np.moveaxis(m, 1, 2)\n",
    "    #l = l.T\n",
    "    #r = r.T\n",
    "    #m = m.T    \n",
    "    inp_mid_shape = m.shape[1]\n",
    "    out_mid_shape = l.shape[1]\n",
    "    middle_adjust = (inp_mid_shape - out_mid_shape) // 2\n",
    "    m = m[:, middle_adjust:-middle_adjust, :]\n",
    "    half = m.shape[1] // 2\n",
    "    lsize = l.shape[1] - half\n",
    "    rsize = lsize + half + half\n",
    "    print(\"Concatenating three feature tiles \", l.shape, r.shape, m.shape)\n",
    "    img = np.concatenate([l, r], axis = 1)\n",
    "    print(\"Concatenated three feature tiles \", img.shape)\n",
    "    sums = np.zeros((img.shape[0], img.shape[1]), dtype = np.float32)\n",
    "    sums[:, :l.shape[0] // 2] = 1\n",
    "    sums[:, l.shape[0] // 2:(l.shape[0] // 2)+half] += (1 - (np.arange(0, half, 1) / half))\n",
    "    sums[:, (l.shape[0] // 2)+half:(l.shape[0] // 2)+half+half] += ((np.arange(0, half, 1) / half))\n",
    "    sums[:, -(r.shape[0] // 2):] = 1.\n",
    "    sums = sums[..., np.newaxis]\n",
    "    sumsright = 1 - sums\n",
    "    img[..., 1:] = img[..., 1:] * sums\n",
    "    img[:, lsize:rsize, 1:] += (m * (1 - sums[:, lsize:rsize]))\n",
    "    print(\"Blended three feature tiles, returning\")\n",
    "    #img[:, lsize:rsize] /= 2,# * (1 - sums[:, lsize:rsize]))\n",
    "    #leftfile = img[:, :l.shape[1]]\n",
    "    #rightfile = img[:, -r.shape[1]:]\n",
    "    #hkl.dump(leftfile, left_local_path, mode='w', compression='gzip')\n",
    "    #hkl.dump(rightfile, right_local_path,  mode='w', compression='gzip')\n",
    "    #uploader.upload(bucket = 'tof-output', key = right_s3_path, file = right_local_path)\n",
    "    #uploader.upload(bucket = 'tof-output', key = left_s3_path, file = left_local_path)\n",
    "    return img, sums\n",
    "\n",
    "\n",
    "def combine_resegmented_feats(x, y, m, indices, base_path = '../project-monitoring/tiles/'):\n",
    "    # Step 1 check if ARD exists\n",
    "    # Step 2 update the \n",
    "\n",
    "    left_local_path = f'{base_path}{str(x)}/{str(y)}/raw/feats/{str(x)}X{str(y)}Y_feats.hkl'\n",
    "    right_local_path = f'{base_path}{str(int(x) + 1)}/{str(y)}/raw/feats/{str(int(x) + 1)}X{str(y)}Y_feats.hkl'\n",
    "    print(\"Loading original features\")\n",
    "    print(left_local_path)\n",
    "    print(right_local_path)\n",
    "    \n",
    "    l = hkl.load(left_local_path) / 32768\n",
    "    r = hkl.load(right_local_path) / 32768\n",
    "    m = m / 32768\n",
    "    l = l.T\n",
    "    r = r.T\n",
    "    m = m.T    \n",
    "    inp_mid_shape = m.shape[1]\n",
    "    out_mid_shape = l.shape[1]\n",
    "    middle_adjust = (inp_mid_shape - out_mid_shape) // 2\n",
    "    m = m[:, middle_adjust:-middle_adjust, :]\n",
    "    half = m.shape[1] // 2\n",
    "    lsize = l.shape[1] - half\n",
    "    rsize = lsize + half + half\n",
    "    print(\"Concatenating three feature tiles \", l.shape, r.shape, m.shape)\n",
    "    img = np.concatenate([l, r], axis = 1)\n",
    "    print(\"Concatenated three feature tiles \", img.shape)\n",
    "    sums = np.zeros((img.shape[0], img.shape[1]), dtype = np.float32)\n",
    "    sums[:, :l.shape[0] // 2] = 1\n",
    "    sums[:, l.shape[0] // 2:(l.shape[0] // 2)+half] += (1 - (np.arange(0, half, 1) / half))\n",
    "    sums[:, (l.shape[0] // 2)+half:(l.shape[0] // 2)+half+half] += ((np.arange(0, half, 1) / half))\n",
    "    sums[:, -(r.shape[0] // 2):] = 1.\n",
    "    sums = sums[..., np.newaxis]\n",
    "    sums = np.repeat(sums[..., np.newaxis], 65, axis = -1).squeeze()\n",
    "    print(f\"The sums are: {sums.shape}\")\n",
    "    print(f\"The img shape is: {img.shape}\")\n",
    "    non_indices = [x for x in np.arange(0, 65) if x not in indices]\n",
    "    print(f\"The non-indices are: {non_indices}\")\n",
    "    sums[..., non_indices] = 1.\n",
    "    #img = img * sums\n",
    "    #img[:, lsize:rsize, 1:] += (m * (1 - sums[:, lsize:rsize, 1:])) # should be 0 for non-adj feats\n",
    "    print(\"Blended three feature tiles\")\n",
    "\n",
    "    img = img.T\n",
    "    img = np.int16(img * 32768)\n",
    "    _left = img[:, :img.shape[1] // 2, :]\n",
    "    _right = img[:, img.shape[1] // 2:, :]\n",
    "\n",
    "    #img[:, lsize:rsize] /= 2,# * (1 - sums[:, lsize:rsize]))\n",
    "    #leftfile = img[:, :l.shape[1]]\n",
    "    #rightfile = img[:, -r.shape[1]:]\n",
    "    print(\"Saving the updated feature tiles and continuing \", _left.shape, _right.shape)\n",
    "    print(f\"The left features are saved to: {left_local_path}\")\n",
    "    print(f\"The right features are saved to: {right_local_path}\")\n",
    "   # hkl.dump(_left, left_local_path, mode='w', compression='gzip')\n",
    "    #hkl.dump(_right, right_local_path,  mode='w', compression='gzip')\n",
    "    #uploader.upload(bucket = 'tof-output', key = right_s3_path, file = right_local_path)\n",
    "    #uploader.upload(bucket = 'tof-output', key = left_s3_path, file = left_local_path)\n",
    "    return img, sums\n",
    "\n",
    "#img2, sums = combine_resegmented_feats(1646, 1128, m, indices = np.arange(0, 65, 1))\n",
    "\n",
    "\n",
    "def combine_resegmented_feats_north(x, y, m, indices, base_path = '../project-monitoring/tiles/'):\n",
    "    # Step 1 check if ARD exists\n",
    "    # Step 2 update the \n",
    "\n",
    "    left_local_path = f'{base_path}{str(x)}/{str(y)}/raw/feats/{str(x)}X{str(y)}Y_feats.hkl'\n",
    "    right_local_path = f'{base_path}{str(int(x))}/{str(int(y) + 1)}/raw/feats/{str(int(x))}X{str(int(y + 1))}Y_feats.hkl'\n",
    "    print(\"Loading original features\")\n",
    "    print(left_local_path)\n",
    "    print(right_local_path)\n",
    "    \n",
    "    l = hkl.load(left_local_path) / 32768\n",
    "    r = hkl.load(right_local_path) / 32768\n",
    "    m = m / 32768\n",
    "    l = l.T\n",
    "    r = r.T\n",
    "    m = m.T    \n",
    "    inp_mid_shape = m.shape[0]\n",
    "    out_mid_shape = l.shape[0]\n",
    "    middle_adjust = (inp_mid_shape - out_mid_shape) // 2\n",
    "    m = m[middle_adjust:-middle_adjust, :]\n",
    "    half = m.shape[0] // 2\n",
    "    lsize = l.shape[0] - half\n",
    "    rsize = lsize + half + half\n",
    "    print(\"Concatenating three feature tiles \", l.shape, r.shape, m.shape)\n",
    "    img = np.concatenate([r, l], axis = 0)\n",
    "    print(\"Concatenated three feature tiles \", img.shape)\n",
    "    sums = np.zeros((img.shape[0], img.shape[1]), dtype = np.float32)\n",
    "    print(sums.shape, l.shape[1] // 2)\n",
    "    sums[:l.shape[1] // 2] = 1\n",
    "    sums[l.shape[1] // 2:(l.shape[1] // 2)+half] += (1 - (np.arange(0, half, 1) / half))[:, np.newaxis]\n",
    "    sums[(l.shape[1] // 2)+half:(l.shape[1] // 2)+half+half] += ((np.arange(0, half, 1) / half))[:, np.newaxis]\n",
    "    rightend = (l.shape[1] // 2)+half+half\n",
    "    print(rightend)\n",
    "    sums[rightend:] = 1.\n",
    "    sums = sums[..., np.newaxis]\n",
    "    sums = np.repeat(sums[..., np.newaxis], 65, axis = -1).squeeze()\n",
    "    print(f\"The sums are: {sums.shape}\")\n",
    "    print(f\"The img shape is: {img.shape}\")\n",
    "    non_indices = [x for x in np.arange(0, 65) if x not in indices]\n",
    "    print(f\"The non-indices are: {non_indices}\")\n",
    "    sums[..., non_indices] = 1.\n",
    "    img = img * sums\n",
    "    print(f\"The image is: {img.shape}, the M is {m.shape}, the sums is {sums.shape}\")\n",
    "    img[lsize:rsize, :, 1:] += (m * (1 - sums[lsize:rsize, :, 1:])) # should be 0 for non-adj feats\n",
    "    print(\"Blended three feature tiles\")\n",
    "\n",
    "    img = img.T\n",
    "    img = np.int16(img * 32768)\n",
    "    print(img.shape)\n",
    "    _right = img[:, :, :img.shape[2] // 2]\n",
    "    _left = img[:, :, -img.shape[2] // 2:]\n",
    "    \n",
    "    #leftfile = img[:, :, :l.shape[1]]\n",
    "    #rightfile = img[:, :, -r.shape[1]:]\n",
    "    print(\"Saving the updated feature tiles and continuing \", _left.shape, _right.shape)\n",
    "    print(f\"The left features are saved to: {left_local_path}\")\n",
    "    print(f\"The right features are saved to: {right_local_path}\")\n",
    "   # hkl.dump(_left, left_local_path, mode='w', compression='gzip')\n",
    "    #hkl.dump(_right, right_local_path,  mode='w', compression='gzip')\n",
    "    #uploader.upload(bucket = 'tof-output', key = right_s3_path, file = right_local_path)\n",
    "    #uploader.upload(bucket = 'tof-output', key = left_s3_path, file = left_local_path)\n",
    "    return _left, _right\n",
    "\n",
    "_left, _right = combine_resegmented_feats_north(1682, 1086, middle, indices = np.arange(0, 65, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6481759-1781-4779-b711-c07198e36fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(_left[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b2557-16fd-43c1-9544-fa9f19d5f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(_right[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417fdf1-b2dc-493b-80cc-6392ae592e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sums[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6e82f-0bf5-4bea-922a-b456a9721945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "right = '../project-monitoring/tiles/1674/1087/raw/feats/1674X1087Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1675/1087/raw/feats/1675X1087Y_feats.hkl'\n",
    "\n",
    "right = '../project-monitoring/tiles/1646/1128/raw/feats/1646X1128Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1647/1128/raw/feats/1647X1128Y_feats.hkl'\n",
    "\n",
    "right = '../project-monitoring/tiles/1672/1082/raw/feats/1672X1082Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1673/1082/raw/feats/1673X1082Y_feats.hkl'\n",
    "\n",
    "#right = '../project-monitoring/tiles/1662/1068/raw/feats/1662X1068Y_feats.hkl'\n",
    "#left = '../project-monitoring/tiles/1663/1068/raw/feats/1663X1068Y_feats.hkl'\n",
    "\n",
    "subprocess.run(['aws', 's3', 'cp', right, 's3://restoration-monitoring/plantation-mapping/data/reseg_feats/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8bee2-4407-4573-9964-bed973844de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "right = '../project-monitoring/tiles/1674/1087/raw/feats/1674X1087Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1675/1087/raw/feats/1675X1087Y_feats.hkl'\n",
    "\n",
    "right = '../project-monitoring/tiles/1646/1128/raw/feats/1646X1128Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1647/1128/raw/feats/1647X1128Y_feats.hkl'\n",
    "\n",
    "right = '../project-monitoring/tiles/1672/1082/raw/feats/1672X1082Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1673/1082/raw/feats/1673X1082Y_feats.hkl'\n",
    "\n",
    "right = '../project-monitoring/tiles/1662/1068/raw/feats/1662X1068Y_feats.hkl'\n",
    "left = '../project-monitoring/tiles/1663/1068/raw/feats/1663X1068Y_feats.hkl'\n",
    "\n",
    "l = hkl.load(left)\n",
    "l = np.moveaxis(l, 1, 2)\n",
    "r = hkl.load(right)\n",
    "r = np.moveaxis(r, 1, 2)\n",
    "img = np.concatenate([r, l], axis = 2)\n",
    "\n",
    "print(l.shape, r.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(0, 65, 1):\n",
    "    plt.figure(figsize =(20, 5))\n",
    "    sns.heatmap(img[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a5996-661a-4f6c-9b01-0b643e5c231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "gain = np.load(\"../src/gaindatesarr.npy\")\n",
    "loss = np.load(\"../src/lossdate.npy\")\n",
    "dates = np.load(\"../src/dates.npy\")\n",
    "print(gain.shape, loss.shape, dates.shape)\n",
    "\n",
    "from datetime import date\n",
    "import datetime\n",
    "\n",
    "d0 = date(2017, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78efba-c443-4e08-9cdc-734332315f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fb373-3df9-4391-ae00-d5d01828ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rs\n",
    "#gtiff = rs.open('../src/example.tif').read(1)\n",
    "multi_year = np.load(\"../src/multi_year.npy\")\n",
    "gtiff = np.median(multi_year, axis = 0)\n",
    "\n",
    "baseline = multi_year[0]\n",
    "sns.heatmap(gtiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8e8db-625c-4b92-865b-471fe1737258",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92060bb-0059-42d7-9433-6392495033cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtiff[np.logical_or(gain > 0, loss > 0)] = baseline[np.logical_or(gain > 0, loss > 0)]\n",
    "gtiff[gtiff > 100] = baseline[gtiff > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad424a7-88da-42c6-bba8-6e1f4aded59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a902b67-0707-48b7-877f-932212b91b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a815de8-328b-4fe1-be3a-32bf0a2832e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "newdata = np.copy(gtiff)\n",
    "for i in range(1, 50):\n",
    "    newdata[gain == i] = np.max(multi_year, axis = 0)[gain == i]\n",
    "    newdata[loss == i] = np.min(multi_year, axis = 0)[loss == i]\n",
    "    p = plt.figure(figsize=(10, 10))\n",
    "    p = sns.heatmap(newdata, vmax = 100, cbar = False, cmap = 'Greens')\n",
    "    p.tick_params(left=False, bottom=False)\n",
    "    p.set(xticklabels=[], yticklabels=[])\n",
    "    p.set_title(d0 + datetime.timedelta(days=int(dates[i])))\n",
    "    plt.savefig(f'{str(i)}.png')\n",
    "    plt.show()\n",
    "    #plt.imsave(f'{str(i)}.png', newdata, cmap = 'Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbc0b5-5cfe-44b2-81a9-a1c8b45329c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
